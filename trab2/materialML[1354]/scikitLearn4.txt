scikit-learn: 
https://scikit-learn.org/stable/user_guide.html

http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html

4. Quarta aula

4.1 Inicialização da Aula

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()


from sklearn import preprocessing #discretizer

import pandas as pd  # dataframe

from sklearn.impute import SimpleImputer  #missing values

from sklearn.experimental import enable_iterative_imputer  #missing values
from sklearn.impute import IterativeImputer  #missing values

4.2. Lidando com dados categoricos/nominais

4.2.1. Com dataframe pandas

df = pd.DataFrame([['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'],['blue', 'XL', 15.3, 'class1']])
df.columns = ['color', 'size', 'price', 'classlabel']
print(df)

size_mapping = {'XL': 3, 'L': 2, 'M': 1}
df['size'] = df['size'].map(size_mapping)
print(df)

print(pd.get_dummies(df[['price', 'color', 'size']]))

4.2.2. Com one-hot encoder
https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features


enc = preprocessing.OrdinalEncoder()
enc.fit(df) 
print(enc.transform(df))

enc = preprocessing.OneHotEncoder()
enc.fit(df) 
print(enc.transform(df).toarray())
print(enc.categories_)

4.3. Discretização (binning)
http://scikit-learn.org/stable/modules/preprocessing.html#discretization	

4.3.1 Intervalos com igual frequencia

est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2, 4], encode='ordinal').fit(X_train)
#est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2, 4], encode='ordinal', strategy='quantile').fit(X_train)
X_bin = est.transform(X_train)
print(X_bin)

4.3.2 Intervalos iguais

est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2, 4], encode='ordinal', strategy='uniform').fit(X_train)
X_bin = est.transform(X_train)
print(X_bin)

4.3.3 Intervalos com agrupamento kmeans sobre a caracteristica

est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2, 4], encode='ordinal', strategy='kmeans').fit(X_train)
X_bin = est.transform(X_train)
print(X_bin)

4.4. Valores ausentes

4.4.1. media com not a number

X = [[1, 2], [np.nan, 3], [7, 6]]
print(X)
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(X)  
X_test = [[np.nan, 2], [6, np.nan], [7, 6]]
print(X_test)
print(imp.transform(X_test))   

4.4.2. media com -1

X = [[1, 2], [-1, 3], [7, 6]]
print(X)
imp = SimpleImputer(missing_values=-1, strategy='mean')
imp.fit(X)  
X_test = [[-1, 2], [6, -1], [7, 6]]
print(X_test)
print(imp.transform(X_test))

4.4.3. moda com not a number

df = pd.DataFrame([["a", "x"],
                   [np.nan, "y"],
                   ["a", np.nan],
                   ["b", "y"]], dtype="category")
imp = SimpleImputer(strategy="most_frequent")
print(imp.fit_transform(df))

4.5. Extracao de características (PCA)

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_r = pca.fit(X_train).transform(X_train)
print(X_train, X_r)

4.6. Selecao de Caracteristicas

4.6.1. Removendo caracteristicas com baixa variância

from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
X_t = sel.fit_transform(X)
print(X_t)

4.6.2. Seleção de Características por Ranking Univariado

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X_b = SelectKBest(chi2, k=2).fit_transform(X_train, y_train)
print (X_b)

4.6.3. Seleção de Características por Busca (Implementar SFS com estimador knn)

4.7. Implementar zeroR e OneR

4.8 Usar com SFS







